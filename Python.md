语言规范
--------

### 程序结构

1.  引用原生库和手写库
2.  定义全局变量
3.  定义修饰器
4.  定义类（包括函数对象）
5.  定义函数（包括`argparse`）
6.  定义主函数

### 易错点

1.  不要将`Python`源码文件命名为包的名字，否则会因重复包含导致错误

### 运行特点

1.  在使用`import`时正确的路径是针对`main.py`而言的，而不是针对当前文件而言的
2.  `global`关键字的使用是为了在局部作用域中引用并修改全局变量
3.  闭包函数若要修改上级作用域中的变量，需要用`nonlocal`关键字
4.  `Python`中只有模块(`module`)，类(`class`)以及函数(`def`、`lambda`)才会引入新的作用域，其它的代码块(如`if`、`try`、`for`等)不会引入新的作用域，因此在代码块外部可以直接引用代码块内声明的变量
5.  `Python`的变量是动态声明的，未考虑到这点可能出现`bug`，例如在`if`语句中声明了变量，若该`if`语句条件为假，则不仅其内部语句不执行，其内部变量也不会被定义。因此，很多时候有必要在`if`语句之前声明变量

### 面向对象

1. 通过装饰器`@staticmethod`定义静态方法，且`@staticmethod`必须写在方法上

函数
--------

### dir

1. `dir`函数可用来查看对象的属性和方法

包
--------

### re

1.  在?、+、\*以及{n,m}后加?表示进行懒惰匹配（与默认的贪婪匹配相反）
2.  `re.sub`方法使用`\g<1>`、`\g<2>`等可以保留被匹配覆盖的部分
3.  使用`(?:)`可以在使用`re.split`将括号内的内容置为非捕获分组，不在结果列表中出现
4.  正则表达式在使用`|`进行“或”的连接时，优先匹配`|`前的部分

### numpy

1.  在`numpy`中，不同函数有的返回的是副本，有的返回的是视图，需要区分，可以用`base`属性来进行查看
2.  `indexing`为视图而`fancy_indexing`为副本

### copy

1. `copy`函数为浅拷贝，只拷贝对象的成员本身，而不拷贝对象成员的成员
2. `deepcopy`函数为深拷贝，会递归的拷贝整个对象

### viztracer

1. `viztracer`可以对`Python`程序进行`profiling`，生成`json`格式的结果文件
2. 使用时既可以执行命令行程序，也可以在程序内插入`viztracer`的代码
3. 若只关心某些目录/文件下的函数，则可以使用命令行中的`--include_files`参数或在初始化`Viztracer`对象时提供`include_files`参数
4. 若只关心某些函数，则可以使用`trace_and_save`修饰器修饰该函数，效果为每次调用该函数时生成一个`json`文件，然后使用`python -m viztracer --combine *.json`来合成一个`json`文件
5. 查看图形化结果需要使用命令`vizviewer result.json`

### torch

1.  `torch`中大写首字母（`Class`）与小写首字母（`Function`）在用法上有区别，功能基本相同
2.  `indexing`为视图而`fancy_indexing`为副本
3.  所有下划线操作直接操作原变量
4.  可以对非叶节点依次使用`detach_`和`requires_grad_`方法，使该非叶节点变为求导的对象
5.  `torch`无法实现对矢量结果的自动求导，需要人为输入一个外部梯度来启动计算
6.  在调用`backward`函数计算梯度之前，要先清空梯度为`0`
7.  `torch.nn`不支持单样本输入，例如`CNN`中卷积函数接受的张量为`4`维（`batch_size`\*`channels`\*`height`\*`weight`）
8.  在不进行训练时，使用神经网络不开启梯度，即使用`with torch.no_grad():`包裹代码
9.  `torch`在使用多`GPU`并行训练时可以自动划分任务并分配给每个`GPU`
10.  `reshape`函数为$O(1)$时间复杂度，可以在代码中大量使用而不会是效率下降
11.  不要使用/创建`1`维的张量（数组），而是以二维张量（矩阵）代替之，否则容易产生意料之外的行为

语法糖
--------

### 推导表达式

1.  推导产生式在`for`循环之后可以接一个`if`
2.  推导产生式可以使用多个`for`循环