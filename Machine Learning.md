Mathematics
-----------

### Calculus

1.  微分矢量公式涉及梯度、散度、旋度的关系，推导较为简单
2.  雅克比矩阵的每一行为向量值一个分量的梯度

### Linear Algebra

1.  `none`

### Probability & Statistics

1.  `none`

Linear Regression
-----------------

### 技巧

1.  特征归一化可以使不同特征的范围较为接近，从而加快收敛速度

logistic regression
-------------------

### 作用

1.  `logistic`函数即$f(x)=\frac{1}{1+e^{-x}}$，可以将自变量映射到$(0,1)$区间内，常用于聚类算法

### 损失函数

1.  一般使用交叉熵函数
2.  以$f(\vec{x})=\frac{1}{1+e^{-\vec{\theta}^T\vec{x}}}$作为模型时，交叉熵函数相比平方平均函数作为损失函数的最外层有两点优势，其一为梯度中不含激活函数的导数项，可以保证较快的收敛速度，其二为可以保证损失函数永远处处为凸（保证二阶偏导矩阵处处半正定）

regularization parameter
------------------------

### 作用

1.  作为在代价函数中模型参数大小的惩罚的参数（例如参数平方和的系数），用于权衡拟合程度
2.  过拟合一般会出现较大参数，调大`regularization parameter`从而防止过拟合，但`regularization parameter`过大会导致所有参数集中于`0`附近，引起欠拟合
3.  只在`train`过程中使用，在`cross validation`和`test`时不使用

Neural Network General
----------------------

### Basic

1.  通过多层网络，实现对现实中复杂函数的模拟
2.  一个`batch`对应一次梯度下降，一个`epoch`对应遍历完一次训练集
3.  `train set`用于训练模型，`cross validation set`用于筛选最优模型，`test set`用于评估泛化误差
4.  `train error`和`cross validation error`接近，说明没有发生过拟合

### BP Algorithm

1.  其推导推导/计算过程为，先计算除输入层外损失函数对每一层神经元接受的输入值的偏导数，可以写成一个列向量（这也是真正使用反向传播计算的量），然后显然有损失函数对任意一个权重的偏导数等于权重边指向的神经元的输入值的偏导数乘上权重边起始的神经元的输出值
2.  考虑正则化参数时要加上正则化参数项的偏导数
3.  对于`batch size`不为`1`时，要计算的损失函数值为平均值，公式有相应修改
4.  根据`BP`的公式，初始的权重要随机初始化，否则会造成同一层的神经元出现完全相同的变化
5.  需要进行梯度检查验证`BP`的正确性

Error Analysis
----------------------

### Methods

1. 在交叉验证过程中进行误差分析
2. 偏斜类指不同结果类别之间实际出现的频率差异过大，导致不能简单的用总体预测准确率来评估模型的预测效果，所以需要利用查准率（真阳性/(真阳性+假阳性)）和召回率（真阳性/(真阳性+假阴性)）来评估
3. 权衡查准率和召回率用调和平均值较为合理（也被称作`F`函数）

Classic Algorithms
----------------------

### SVM

1. `SVM`通过调整激活函数，可以得到更加鲁棒的分类效果，即将正负样本尽量以大间距分开
2. 核函数可用来提取更多、更本质的特征，适用于样本数多于特征数的情况

### K-means

1. `k-means`用于无监督学习中的聚类算法，需要随机初始化防止落入局部最优，且算法保证迭代过程中损失函数（所有样本点到自身所属（欧式距离最近）的聚类中心点的距离之和）单调下降
2. 在选择合适的聚类种类数时，除了根据实际问题的需求外，`k-means`可以通过肘部（损失函数终态值随聚类种类数的变化曲线突然变“缓”的位置）判断合适的聚类种类数

### PCA

1. `PCA`用于可用于数据降维（压缩），保证投影损失最小
2. `PCA`不适合用于防止过拟合
3. 在原始方法效果不好的情况下再尝试使用`PCA`进行优化

### Anomaly Detection

1. 