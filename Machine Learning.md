Mathematics
-----------

### Calculus

1.  微分矢量公式涉及梯度、散度、旋度的关系，可以通过计算进行推导

2.  雅克比矩阵的每一行为向量值一个分量的梯度

### Linear Algebra

1.  `none`

### Probability & Statistics

1.  `none`

Linear Regression
-----------------

### 技巧

1.  特征归一化可以使不同特征的范围较为接近，从而加快收敛速度

logistic regression
-------------------

### 作用

1.  `logistic`函数即$f(x)=\frac{1}{1+e^{-x}}$，可以将自变量映射到$(0,1)$区间内，常用于聚类算法

### 损失函数

1.  一般使用交叉熵函数

2.  以$f(\vec{x})=\frac{1}{1+e^{-\vec{\theta}^T\vec{x}}}$作为模型时，交叉熵函数相比平方平均函数作为损失函数的最外层有两点优势，其一为不含`logistic`函数的导数项，可以保证较快的收敛速度，其二为可以保证损失函数永远处处为凸（保证`Hessian`矩阵一定处处半正定）

regularization parameter
------------------------

### 作用

1.  作为在代价函数中模型参数大小的惩罚的参数（例如参数平方和的系数），用于权衡拟合程度

2.  过拟合一般会出现较大参数，调大`regularization parameter`从而防止过拟合，但`regularization parameter`过大会导致所有参数集中于`0`附近，引起欠拟合
    
3.  只在`train`过程中使用，在`cross validation`和`test`时不使用

Neural Network General
----------------------

### Basic

1.  通过多层网络，实现对现实中复杂函数的模拟

2.  一个`batch`对应一次梯度下降，一个`epoch`对应遍历完一次训练集

3.  `train set`用于训练模型，`cross validation set`用于筛选最优模型，`test set`用于评估泛化误差
    
4.  `train error`和`cross validation error`接近，说明没有发生过拟合

### BP Algorithm

1.  其推导推导/计算过程为，先计算除输入层外损失函数对每一层神经元接受的输入值的偏导数，可以写成一个列向量（这也是真正使用反向传播计算的量），然后显然有损失函数对任意一个权重的偏导数等于权重边指向的神经元的输入值的偏导数乘上权重边起始的神经元的输出值

2.  考虑正则化参数时要加上正则化参数项的偏导数

3.  对于`batch size`不为`1`时，要计算的损失函数值为平均值，公式有相应修改

4.  根据`BP`的公式，初始的权重要随机初始化，否则会造成同一层的神经元出现完全相同的变化

5.  需要进行梯度检查验证`BP`的正确性